---
tags:
  - 考研
  - math
  - book
---
# 随机事件与概率

可描述的，不确定的、重复的试验带来的结果使用概率描述。对于所有的可能结果，我们使用样本空间描述概括：

- 包含所有的结果，完备
- 有限或者可列个元素
- 元素为样本点

随机事件：

1. 随机事件是样本空间的自己，每个事件可以由一个或者多个 **样本点** 组成
2. 随机事件在试验之前无法确定是否发生
3. 可以被分类。他是简单事件的复合。（这是我们解决后面大多数问题的思路）

必然事件：包含样本空间中所有样本点的事件。不可能事件：不包含任何样本点的事件。

事件是概率的本质，我们期望在 $\sigma$ 代数上定义他们之间的运算。 但是概率是无法推定事件的！。$P = 0$ 不意味着不可能事件。这是由于我们的实数域上思考连续概率问题时，所定义的连续性所带来的。 研究事件之间的关系，首先给出他们的定义：

1. 包含关系。$\emptyset \subseteq A \subseteq \Omega$ ，体现在 : $AB = A,  \iff A \subseteq B$ 
2. 相等，即互相包含 $A \subseteq B, B \subseteq A$   
3. 互斥 $A \cap B = \emptyset$ 
4. 对立：$A \cap B = \emptyset, A \cup B = \Omega$ 

注意区分对立和互斥。进一步的我们就能够定义他们之间的运算：

事件的和，事件对应的样本点的汇总。 $A \subseteq A \cup B, B \subseteq A \cup B$  

事件的交，事件对应的样本点的重叠部分，$A \cap B = \{ \omega| \omega \in A, \omega \in B \}$  。进一步的我们有 $A \cap B \subseteq A, A \cap B \subseteq B$ 

事件的逆，也就是从全部的样本空间中去除掉本身。$A \cap \bar{A} = \emptyset, A \cup \bar{A} = \Omega$ 

事件的差，一个事件对应的样本点去掉会发生另一个事件的样本点 $A - B = A\cap \bar{B}, A - B \subseteq A$

进一步的，我们可以说明计算中的运算律：

1. 吸收律： $A \subseteq B \to A \cup B = B, A \cap B = A$ 吸收律非常有效的消除了其它的变量，这在复杂运算中非常好用。特别是关于一些事件的运算。
2. 交换律：$A \cup B = B \cup A, A \cap B = B \cap A$ 
3. 结合律，只能同一种运算
$$
A \cup B \cup C = (A \cup B) \cup C = A \cup ( B \cup C)
$$
4. 分配律，

$$
A \cup ( B \cap C) = ( A \cup B) \cap (A \cup C); A \cap ( B \cup C) = ( A \cap B) \cup (A \cap C)
$$

5. 对偶律：
$$
\bar{A \cup B} = \bar{A} \cap \bar{B}; \bar{ A \cap B} = \bar{A} \cup \bar{B}
$$

随机事件的概率，我们明确其非负性、规范性、可列可加性。

$$
P(\emptyset) = 0, P(\Omega) =1, 0 \le P(A) \le 1; P(A_{1} \cup A_{2} \cup\dots \cup A_{n}) = P(A_{1}) + P(A_{2}) + \dots + P(A_{n})
$$

运算定义如下：

$P(A \cup B) = P(A) + P(B) - P(AB)$ 


$P(\bar{A}) = 1 - P(A)$ 

$P(A - B) = P(A) - P(AB) = P(B) - P(AB) \overset{B \subset A}{=}P(A) -P(B)$ 
$P(A - B) = P(A \bar{B})$ 

$P(AB) = P(A)P(B|A)$

> 概率只能说明样本点的测度，但是无法反推事件的性质与关系

## 古典概型与几何概型

样本空间只包含有限个样本点。每个样本发生的概率相同。

对于事件A:

$$
P(A) = \frac{\text{事件A中的样本点}}{\text{样本空间中的样本总数}}
$$

进一步的我们可以用面积长度等等来作为事件多少的测度，这就是几何概型。


## 条件概率

$P(B|A) = \frac{P(AB)}{P(A)}$

这是直觉成立的公式。普通概率就是 $P(A) = P(A|\Omega)$ 因此条件概率的计算符合之前的所有公式。

在计算上我们说明之前吸收律的用处，如果 $A \subseteq B$ 那么我们有;

$$
P(A|B) = \frac{P(AB)}{P(B)} = \frac{P(A)}{P(B)}
$$

## 全概率公式、贝叶斯公式

正如我们之前所说的$P(A) = P(A|\Omega)$ 如何从这个角度，将$\Omega$ 下 并不容易计算$P(A)$   进行转换? 我们首先定义完备事件组：

$$
A_{i}A_{j} = \emptyset, A_{1} \cup A_{2} \dots \cup A_{n} = \Omega
$$

那么我们就可以说：

$P(A|\Omega) = \sum P(A_{i})P(A|A_{i})$ 

进一步的我们有：

$P(A_{i}|A) = \frac{P(A_{i}A)}{P(A)} = \frac{P(A|A_{i}P(A_{o}))}{\sum P(A_{i})P(A|A_{i})}$ 


## 概率的计算

设 $A,B,C$ 为三个随机事件，且 $P(A) = P(B) = P(C) =\frac{1}{4}$  ,且：

$$
P(AC) = P(BC) = \frac{1}{12}, P(AB) = 0
$$

求 $A,B,C$ 中至少有一个事件发生的前提下，恰好有两个事件发生的概率。

转化为事件 $P(\bar{A}BC + A \bar{B} C + AB \bar{C} |A \cup B \cup C)$ , 我们观察到了 恰好发生两个事件，是 至少有一个事件发生的子集因此：

$$
\begin{align}
 & \frac{P(\bar{A}BC + A \bar{B} C + AB \bar{C})}{P(A \cup B \cup C)}  \\
= &  \frac{P(\bar{A} BC + A \bar{B} C)}{P(A \cup B \cup C)}  \\
= & \frac{P(\bar{A}BC) + P(A \bar{B} C)}{P(A \cup B \cup C)} \\
=  &  \frac{P(BC) - P(BCA) + P(AC) - P(ABC)}{P(A) + P(B) + P(C) - P(BC) - P(AC) - P(AB) + P(ABC)}
\end{align}
$$

$n$ 个人抓 $m$ 个 有物之 阄  ，则 k 个人抓到的概率为 :

也就是第 $K$ 次才抓到的概率： $\frac{m}{n}$ 这是一个 和 $k$ 无关的。

## 事件独立

对于 $A$ 与 $B$ 独立 我们有 $A$ 与 $\bar{B}$ 独立， $\bar{A}$ 与 $B$ 独立， $\bar{A}$ 与 $\bar{B}$ 独立。根据独立，我们有：

$$
P(AB) = P(A)P(B)
$$

反之则不成立。进一步的对于三个事件，我们有相互独立，两两独立：

如果$P(AB) = P(A)P(B);P(AC) =P(A)P(C); P(BC)=P(B)P(C)$则他们两两独立。进一步的如果还有 :

$P(ABC) = P(A)P(B)P(C)$ 

则称相互独立。

## 独立可重复试验和伯努利概型

伯努利试验只有两个结果，$A$ 和 $\bar{A}$ 进一步的如果每次试验相互独立，称为独立试验，将一个伯努利试验独立重复的进行（这意味着有放回）：

事件 A 发生的概率为 $p$ 那么恰好发生 $k$ 次的概率为：

$$
C_{n}^{k} p^{k}(1-p)^{n-k}
$$

# 随机变量及其分布

概率密度函数和分布函数是这里的重点，掌握他们的性质是判断的关键。理解他们之间的转换关系。对于离散随机变量能够根据其分布律写出分布函数。

并且我们需要记忆典型的分布比如泊松分布、指数分布等、连续相比离散更为重要。最后我们要探讨连续随机变量函数的概率密度和分布函数。

实际上对于连续随机变量而言，我们有 $P(X =x) = 0$ 这也就意味着对于连续随机变量而言，分布函数必然连续。但是对于离散随机变量，显然这是不成立的。进一步的我们规定了 $F(x) = P(X \le x)$ 这就确定了：

- 分布函数是一个右连续的函数  ，对于分段点左闭右开
- $0 \le F(x) \le 1, F(-\infty) = 0, F(+\infty) = 1$  
- $F(x)$ 是单调不减的函数  

对于离散随机变量我们需要特别关注于分段点。

## 分布函数和概率密度函数、分布律

这里我们主要说明离散随机变量如何从分布律得到我们的分布函数：

$P\{ X =-1 \} = \frac{1}{5}, P\{ X =1 \} = \frac{2}{5}$ 随机变量 $X$ 的 绝对值不大于1。在事件 $\{ -1 < X < 1 \}$ 的条件下，$X$    在 $(0,1)$ 内的任一子区间上的取值的条件概率与该子区间长度成正比。 

混合随机变量，我们分离出分段点：-1 和 1 两个端点。我们有：

$x < -1, F(x) = P(X \le x) = 0$
$-1 \le x < 1, \quad F(x) = P(X \le -1) +  P(X = -1) + P(-1< X \le x)$  题目中给出了：

$$
P\{ -1 < x \le x |-1 < x < 1 \} \overset{\subseteq}{=} \frac{P\{ -1 < X \le x \}}{P\{ -1 < x <1 \}} = \frac{x - -1}{1--1} = \frac{x+1}{2}
$$

又有完备性 $P\{ -1 < x <1 \} = 1- \frac{2}{5} - \frac{1}{5} = \frac{2}{5}$ ,  因此我们的 $F(x) = 0 +\frac{1}{5} + \frac{x+1}{5}$ 

$1 \le x,  F(x) = 1$ 

综上：

$$
F(x) = \begin{cases}
0 & x <-1  \\
\frac{x+2}{5} & -1 \le x < 1 \\
1 & x >1
\end{cases}
$$

进一步的我们需要强调分布函数的几个小细节：

> [!note] 分布函数
> 1.左闭右开的区间
> 2.完备性，能够覆盖所有的样本空间（这在二元随机变量中非常的重要，因为非常有可能遗漏）
> 3. 规范性 从0到1,单调不减
## 离散分布

离散随机变量的分布律可以简单的差分得到，由于是右连续因此减去左极限得到这一点的原子质量。
$$
P\{ X = x \} = F(x) - F(x^{-})
$$

分布函数注意分段点累计得到。

我们先说明一些常见的离散分布：

- 0,1分布 。进行一次的二项分布
- 二项分布 $P(X =k) = C_{n}^{k} p(k) (1-p)^{1-k}$ 
- 泊松分布，$X \sim P(\lambda) \le P(x = k) = \frac{\lambda ^{k}}{k!}e^{-\lambda}$  也就是 $e^{\lambda}$ 的幂级数展开的每一项乘以我们的 

二项分布可以由我们的泊松分布近似：

$$
P(X =k ) \sim \frac{\lambda ^{k}e^{-\lambda}}{k!}, iif 试验次数很大成功概率低 \iff \lim_{ n \to +\infty } C_{n}^{k}p(k)(1-p)^{n-k} = \frac{\lambda ^{k}e^{-\lambda}}{k!}
$$

泊松分布是指数分布的离散版本，他们也是一元随机变量分布的重点。


- 几何分布：$P(X =k) = (1-p)^{k-1}p, k =1,2\dots$  。等待一次成功，最后一次成功前面全部失败。他有一个性质：无记忆性质。因为他只关心最后一次，因此 $P(X > m+n|X >m) = P(X>n)$ 
- 超几何分布。对应不放回抽样。

> [!note] 超几何分布与二项式分布的区别
> 超几何抽样是不放回的，而二项分布是放回的
> 进一步的，如果 总数$N$ 非常大，并且 我们期望的 $n$ 非常小，放不放回几乎可以忽略时，超几何分布和二项分布是近似的。

## 连续分布

对于有概率密度函数的连续随机变量，分布函数是连续！因为每一点的概率取值是0。对于分布函数求概率密度的求导，我们可以简化，开区间直接求，间断点直接取0即可。

常见的连续随机变量的分布如下：

1. 均匀分布
2. 指数分布
$$
f(x) = \begin{cases}
\lambda e^{-\lambda x}  &  x> 0 \\
0 & x \le 0
\end{cases}
$$

分布函数

$$
F(x) = \begin{cases}
0 ,x < 0 \\
1- e^{\lambda x} x \le 0
\end{cases}
$$

这里要倒背如流，这是因为我们后续算数字特征的时候经常遇到积分复杂的情况比如：

$$
\int f(x) e^{-3t}
$$

那我们就需要凑到指数分布，然后利用它的数字特征进行求解。！！！

- 正态分布

$$
f(x) = \frac{1}{\sqrt{ 2 \pi }\sigma} e^{-\frac{(x-\mu)^{2}}{2\sigma ^{2}}}
$$

这也需要倒背如流，因为我们的做题过程就是凑出一个正态分布。分三步：

1. 配方 把 指数上面的配方了
2. 除系数 ，根据指数配方结果 凑出 $\sigma$ 
3. 添因子，根据上面解出来的 $\sigma$ 我们在前面添加上因子。

正态分布的性质：

1. 标准化： $X \sim N(\mu,\sigma ^{2})$   其分布函数为$F(x)$ 则， $\frac{X - \mu}{\sigma} \sim N(0,1)$

$$
F(x) = \Phi\left( \frac{X - \mu}{\sigma} \right)
$$

$$
P \{ a < x \le b \} = \Phi\left( \frac{b-\mu}{\sigma} \right) - \Phi\left( \frac{a - \mu}{\sigma} \right) 
$$

- 对称性

$$
\Phi(-a) = 1 - \Phi(a), \Phi(0) = \frac{1}{2}
$$

正态函数题目都好回归到标准正态以及他的对称性上。因此就和我们上面说到的，需要凑到标准正态，那么也就是三个步骤。
## 随机变量函数

如果 $X$ 是一个随机变量，并且我们得到了他的pdf 或者 pmf 以及 cdf 如何来说明 $g(X)$?

对于离散随机变量，我们可以列出分布律，列举得到 $P\{ g(X) = x \}$ 的 情况。进一步的得到他的分布函数。对于对于都是连续型随机变量。我们考虑定义法和公式法：

1. 定义法，

$$
F_{Y}(y) = P\{ Y \le y \} = P(g(X) \le y) = \int _{g(x) \le y} f_{X}(x) dx
$$

> [!note] 定义法步骤
> 如何理解这个积分计算的面积区域是关键：
> 1. 绘制出 $g(X)$ 的图像， 其中 $y$ 视为常数，更具图像找到$y$ 的分类点。
> 2. 求解$x \in I, s.t.$ $g(x) \le y$  
> 3. 计算 $\int _{x \in I}f_{X}(x) dx$ 


2. 公式法。公式法的要求非常苛刻。需要一个单调的函数映射关系： 我们可以得到 $x = h(y)$ 

$$
F_{Y}(y)= \int _{-\infty}^{h(y)} f_{X}(h(y)) dy 
$$

求概率密度则需要求导，变限积分求导就到了

$$
f_{Y}(y) = \begin{cases}
f_{X}[h(y)]\left| h'(y) \right|  &  \alpha < y < \beta \\
0 & others
\end{cases}
$$

其中 $(\alpha,\beta)$ 是值域。

# 多维随机变量及其分布


> [!outline] 重点
> - 联合分布函数，边缘分布函数
> - 边缘概率密度函数，条件概率密度函数
> - 二维正态分布。
> - 二维随机变量的独立性
> - 二维随机变量函数的分布
> - min max 型 随机变量的分布

## 联合分布函数和边缘分布函数

X，Y是定义在同一样本空间上的两个随机变量。我们研究他们之间的关系，也就是：

$$
F(x,y) = P\{ X \le x \cap Y \le y \} 
$$

联合分布函数就是对应的：

$$
F(x,y) = P\{ X \le x, Y \le y \} 
$$

重要的性质：

-  $0<F(x,y)<1$ , $F(a,-\infty) = 0$ $F(-\infty,b) = 0$, $F(+\infty,+\infty) = 1$   
- 单调不减和右连续
- 二维随机变量落在矩形区域 $D = \{ (X,Y)| x_{1} < X \le x_{2}, y_{1} < Y \le y_{2} \}$  为：

$$
P\{ x_{1} < X \le X_{2}, y_{1} < Y \le y_{2} \} = F(x_{2},y_{2}) - F(x_{1},y_{2}) - F(x_{2},y_{1}) + F(x_{1},y_{1})
$$

知道了联合分布函数我们可以轻松的算出 各个变量的分布函数，也就是边缘分布函数：

$$
F_{X}(x) = P\{ X \le x \} = P \{ X \le x, -\infty < y < +\infty \} = \lim_{ y \to +\infty } F(x,y)
$$
## 联合分布函数和边缘概率密度和条件概率

概率密度部分，对于二元离散型随机变量而言我们主要使用分布律，画出分布律的步骤时：

1. 列表格
2. 已知条件作为约束填入，特别是边缘的概率
3. 验算结果

对于二元连续型随机变量，联合概率密度表示为：

$$
 F(x,y) = \int _{-\infty}^{y} \int _{-\infty}^{x} f(x,y) dx dy
$$

其中 $f(x,y)$ 就是 $(X,Y)$  的联合概率密度。进一步的，边缘概率密度可以被定义为：

$$
f_{X}(x) = \int _{-\infty}^{+\infty} f(x,y) dy
$$

那么边缘分布就是：

$$
F_{X}(x) = \int _{-\infty}^{x} \int _{-\infty}^{+\infty} f(x,y) dy dx
$$


当 $f_{Y}(y) \neq 0$ 条件概率密度就可以被相应的计算：

$$
f_{X|Y}(x|y) = \frac{f(x,y)}{f_{Y}(y)}
$$

当我们同时知道了，条件概率密度和边缘概率密度，我们可以算出 $f(x,y)$ ，从而计算出总体的联合分布。

这是 $Y = y$ 时，X的条件概率密度，我们可以进一步的给出他的分布函数：

$$
F_{X|Y}(x|y) = \int _{-\infty}^{+\infty} \frac{f(x,y)}{f_{Y}(y)} dx
$$


下面我们就可以说明，联合分布中二维随机变量的独立性：

$$
F(x,y) =F_{X}(x)F_{Y}(y)
$$

也就是：

$$
P\{ X \le x, Y \le y \} = P\{ X \le x \} P\{ Y \le y \} \tag{3.1}
$$

这里实际上蕴含了两点必要条件，第一我们的联合分布，取值的区域应该是个矩形。不然在二维坐标系上无法给出这样的恒等关系。第二，联合分布函数能够被分离成两个函数相乘的型式，$g(x)f(y)$ 

对于离散型随机变量而言，(3.1) 等价于：

$$
P(X = x_{i}, Y = y_{j}) = P\{ X = x_{i} \} P\{ Y=y_{j} \}
$$

对于连续随机变量而言则要求：

$$
f(x,y) = f(x) f(y)
$$

变量独立的条件十分严苛但是也带来了非常好的性质：

1. $f_{Y|X}(y|x) = f_{Y}(y)$ 
2. 如果 X 和 Y 独立，那么 $g(x),f(y)$ 也独立。 
3. 如果 $X_{1},X_{2},\dots,X_{n}$ 相互独立，那么对于任意集合 $A_{1},A_{2},\dots,A_{n}$ 只要 $A_{i} \cap A_{j} =\varphi(1 \le i \neq j \le n)$   

$$
P\{ \cap _{i=1}^{n} X_{i} \in A_{i} \} = \prod _{i=1}^{n} P \{ X_{i} \in A_{i} \}
$$

## 二维均匀分布和二维正态分布

下面介绍典型分布：二维均匀分布，二维正态分布

$$
f_{X,Y}(x,y) = \begin{cases}
\frac{1}{S(D)},  & (x,y)\in D \\
0 , & others
\end{cases}
$$

均匀分布的概率密度函数如上所示，非常自然，每个点的概率密度是面积的倒数。他有一些平凡的性质：

1. 对于任意子区域 $A \subseteq D$ (X,Y)  落入 A 的 概率与 A 的 面积呈正比：

$$
P\{ (X,Y) \in A \}  = \frac{S(A)}{S(D)}
$$

2.  X,Y是否独立与区域D的形状密切相关。如果区域D是个矩形，那么这两个变量的边缘分布为一维均匀分布：
$$
X \sim U(a,b), Y \sim U(c,d)
$$
3. 如果 X，Y均匀分布，那么对X，Y的线性变换后得到的新随机变量 $(U,V)$  也是均匀分布。

$$
f(x,y) = \frac{1}{2\pi \sigma_{1}\sigma_{2} \sqrt{ 1-\rho ^{2} }} \exp \left\{  -\frac{1}{2(1-\rho ^{2})} \left[ \frac{(x-u_{1})^{2}}{\sigma_{1}^{2}} - 2\rho \frac{(x -\mu_{1})(y-\mu_{2})}{\sigma_{1}\sigma_{2}} + \frac{(y - \mu_{2})^{2}}{\sigma_{2}^{2}} \right]  \right\} 
$$

二维正态分布函数的概率密度如上所示。我们记 $(X,Y)$ 服从 二维正态分布 $N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$   其中 $\sigma_{1} > 0,\sigma_{2} > 0, \left| \rho \right| <1$ 

- 两个边缘分布为正态分布 $X \sim N(\mu_{1},\sigma_{1}^{2}), Y \sim N(\mu_{2},\sigma_{2}^{2})$ ，反之不一定成立
- 他们的非0线性组合也是正态分布： $aX + bY \sim N(a\mu_{1}+b\mu_{2},a^{2}\sigma_{1}^{2}+ b^{2}\sigma_{2}^{2})$ 
- X与Y相互独立 $\iff$ $\rho=0$ 
- X 和 Y的非0线性组合 $(aX + bY,cX+dY)$ 也是二维正态分布

## 二维随机变量函数的分布

二维随机变量函数的分布对于离散来说仍然是平凡的。但是对于我们的连续随机变量来说情况有所改变。我们记 $g(X,Y) = Z$ , $P\{ Z \le z \} = P\{ g(X,Y) \le z \}$ 

这个时候我们当然也有定义法，但是涉及到三个变量的分段讨论，这使得他的适用性大幅下降。

$$
F_{Z}(z) = \iint _{g(x,y)\le z} f(x,y)dxdy
$$

并且我们还需要计算带有三个字母的二重积分，难度非常大，只有当非常容易使用极坐标换元的特征明显时我们考虑定义法。因此这里我们首先考虑公式卷积法，先算出概率密度然后在得到我们的分布函数。

如果 $Z = g(X,Y)$ 是关于 X，Y的线性函数，我们必然能够分离出： $Y = h(X,Z)$ 或者 $X = s(Y,Z)$ 那么我们计算可以为：

$$
f_{Z}(z) = \int _{-\infty}^{+\infty} f(x,h(x,z)) \left| \frac{\partial h(x,z)}{\partial x} \right|dx 
$$

这里的关键是分段，清晰的到 $x,h(x,z)$ 的全部取值的可能性，在不同范围中使用计算出来的 $f(x,h(x,z))$  来计算。

对于混合型的随机变量函数的分布：

$$
F(z) = P\{ Z \le z \} = P(g(X,Y) \le z) = \sum _{i}P\{ X = x_{i},g(x_{i},Y) \le z \}
$$

显然，这时候我们无法使用卷积法计算。必须要通过定义法，而离散随机变量 $X$ 能够帮助降低大半的麻烦。此时实际上我们计算的是一维随机变量。 也就是计算 $P\{ g(x_{i},Y) \le z \}$ ，这可以由 $Y$ 的概率密度或者分布函数得到。

## min-max 型函数

进一步的我们来说明非常常见的，函数映射 $min\max$ ，这一部分的关键在于需要明确是否独立！！

$$
F_{max} = P\{ max(X,Y) \le z \} = P\{ X \le z, Y \le z \}  = F(z,z),
$$

如果 $X,Y$ 相互独立，则 $F_{max}(z)= F_{X}(z)F_{Y}(z)$  

$$
F_{min} = P\{ min(X,Y) \le z \} = P\{ X \le z \cup Y \le z \} = P\{ X \le z \} + P\{ Y \le z\} - P\{ X \le z \}P\{ Y \le z \}
$$

如果 X，Y相互独立，则 $F_{min}(z) = 1 - \left[ 1 - F_{X}(z) \right]\left[ 1- F_{Y}(z) \right]$ 

$$
U = max\{ X,Y \} = \frac{1}{2}\left( X+Y + |X-Y|\right) \quad
V = min \{ X,Y \} = \frac{1}{2} \left( X+Y - |X-Y| \right) 
$$
并且我们有 $U+V = X+Y, U\cdot V = X\cdot Y$ 

进一步的，如果X，Y相互独立又服从一个分布，那么我们有：

- $X \sim B(n,p), Y \sim B(m,p)$  ， $X + Y \sim B(n+m,p)$ 
- $X \sim P(\lambda_{1}), Y \sim P(\lambda_{2}), X + Y \sim P(\lambda_{1}+\lambda_{2})$ 
- $X \sim N(\mu_{1},\sigma_{1}^{2}),Y \sim N(\mu_{2},\sigma_{2}^{2}),X+Y \sim N(\mu_{1}+\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2})$ 
# 随机变量的数字特征

这里主要是掌握：期望、方差、协方差、相关系数的性质。以及常见分布的数字特征与参数的关系。协方差是这里的关键，他能够串联起十分充足的考点。

## 数学期望

离散型随机变量的数学期望是，$\sum _{i} x_{i}p_{i}$  值，当使存在。对于连续随机变量而言就是 $\int _{-\infty}^{+\infty} xf(x) dx$  。这两个都需要绝对收敛才行。

> [!note] 数学期望的性质
> 1.$E(C) = C$
> 2. $E(CX) = CE(X)$  
> 3. $E(X) \pm E(Y) = E(X\pm Y)$ 
> 4. $E(a_{1}x_{1}+a_{2}X_{2}+\dots+a_{n} X_{n}) = a_{1}E(X_{1}) + a_{2}E(X_{2})+ \dots+ a_{n}E(X_{n})$ 
> 5. 如果X，Y相互独立，那么 $E(XY) = E(X) - E(Y)$ 
> 6. $X \le \to E(X) \le a$ 

## 方差

进一步的，我们定义方差：$D(X) = E[(E(X)-X)^{2}]$ ，这里最重要的公式就是：

$$
D(X) = E(X^{2}) - (E(X))^{2}
$$

也就是;

$$
E(x^{2}) = D(X) + (E(X))^{2} 
$$

> [!note] 方差的性质
> 1.$D(C) = 0; D(X)=0 \iff P\{ X =E(X) \} = 1$ 
> 2. $D(aX + b) = a^{2}D(x)$ 
> 3. $D(X\pm Y) = D(X) + D(Y) \pm 2 Cov(X,Y), Cov(X,Y) = E(XY) - E(X)E(Y)$ 
> 4. $E[(X-t)^{2}] \ge D(x) = E[X-E(X)]^{2}$ 


> [!memory] 常见分布的期望和方差
> $$
> \begin{matrix} 
> X \sim B(1,p)  & E(X) = p & D(X) = p(1-p) \\
> X \sim B(n,p) & E(X) = np  &  D(x) = np(1-p) \\
> X \sim P(\lambda) &  E(X) = \lambda & D(X) = \lambda \\
> X \sim U(a,b) & E(X) = \frac{a+b}{2} & D(X) = \frac{(b-a)^{2}}{12}  \\
> X \sim E(\lambda)  & E(X) = \frac{1}{\lambda}  & D(X) = \frac{1}{\lambda ^{2}} \\
> X \sim N(\mu,\sigma ^{2}) & E(X) = \mu & D(X) = \lambda \\
> X \sim G(p) & E(X) = \frac{1}{p} & D(X) = \frac{1-p}{p^{2}} \\
> X \sim H(N,M,n)  &  E(X) = n \frac{M}{N}  & n \frac{M}{N} \left( 1- \frac{M}{N} \right)\left(  \frac{N-m}{N-1} \right)
> \end{matrix}
> $$

## 协方差和相关系数

协方差是两个随机变量之间的关系，从方差公式变形而来：

$$
Cov(X,Y) = E\{ [X -E(X)][Y - E(Y)] \} = E(XY) - E(X)E(Y)
$$

相关系数是标准化到 0,1上的：

$$
\rho _{XY} = \frac{Cov(X,Y)}{\sqrt{ D(X) } \sqrt{ D(Y) }}
$$

> [!note] 协方差的性质
> 1. $Cov(X,Y) = Cov(Y,X)$
> 2. $Cov(X,X) = D(X)$
> 3. $Cov(X,c) = 0$ 
> 4. $Cov(aX,bY) = ab Cov(X,Y)$ 
> 5. $Cov(X_{1}+X_{2},Y) = Cov(X_{1},Y)+ Cov(X_{2},Y)$ 
> 6. $Cov(X_{1}+ X_{2},Y_{1}+Y_{2}) = Cov(X_{1},Y_{1})+Cov(X_{1},Y_{2})+Cov(X_{2},Y_{1})+ Cov(X_{2},Y_{2})$ 
> 7. 如果X，Y独立则有 $Cov(X,Y) = 0$ 

> [!note] 相关系数和独立
>-  $Y = aX + b \to P\{ Y = aX +b \} = 1 \iff |\rho _{XY}| =1$ 但是数字特征也同样的无法推出概率 但是事件可以轻松地得到我们的相关系数。
>- 随机变量 X与Y相互独立，则一定不相关
>- 如果随机变量 X与Y相关，则一定不独立
>- 如果随机变量X与Y不相关，无法判断是否独立。
>- 如果 X与 Y服从的是二维正态分布，则$\rho _{XY} = 0  \iff$ X 与 Y 独立 

$$
\rho _{XY} = 0 \iff Cov(X,Y) = 0 \iff E(XY) = E(X)E(Y) \iff D(X \pm Y) = D(X) + D(Y)
$$


计算数字特征，重点在于 协方差，看到了相关系数就应该写出协方差方差的式子。

随机变量的矩阵

K阶原点矩和K+L的混合原点矩：

$$
E(x^{k}); E(X^{k}X^{l})
$$

K阶中心矩和K+L的混合中心矩：

$$
E\{ [x - E]^{k} \} ; E\{ [x-E]^{k} [x -E]^{l} \}
$$



# 大数定律和中心极限定理

> 要熟记这些定律和定理的应用范围即可

依概率收敛：

$$
\lim_{ n \to \infty } P \{ \left| X_{n} -a \right| < \varepsilon \} =1 
$$

只要求概率上逐渐接近与1,也就意味着并不要求每个样本路径上都接近。

## 切比雪夫不等式

数学期望和方差存在，对于任意的 $\epsilon > 0$ 我们有：

$$
P\{ |X - E(X)| \ge \epsilon \} \le \frac{D(X)}{\epsilon ^{2}} , P\{ |X - E(X)|  < \epsilon\}\ge 1- \frac{D(X)}{\epsilon ^{2}} 
$$

## 切比雪夫大数定律

## 辛钦大数定律

## 伯努利大数定律

# 数理统计的基本概念


# 点估计和估计量标准























